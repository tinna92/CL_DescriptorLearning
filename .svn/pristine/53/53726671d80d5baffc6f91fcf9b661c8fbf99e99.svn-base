test the influence of threshold l_pull and l_push


firstly, the following test could be done:
1) How l_pull and l_push influence the performance of the learning descriptor?
test the following cases:
l_pull = 0, l_push = 10 11.5% (without mining)
l_pull = 1, l_push = 10 6.4% (0,10 for sort and 1,10 for mining)
l_pull = 3, l_push = 10 8.6%
l_pull = 5, l_push = 10 10.1%
l_pull = 7, l_push = 10 


(Train: Liberty 0,10 for sort  (1,10 with mining) ,mining to 125 (from 1000))
test : Notreme 6.4%, Yosemite 10.9%) without saving theta,
then test mining with 0 and 10.

Train:Notredeme, test Yosemite 5， 10 without mining 11.6%
Train:Notredeme, test Liberty 5， 10 without mining 11.6% (yes, the same)

(save('learnedParamaters0319_30_batchsize_1000_noterme_validation.mat','theta');
save('Save_v_t_r_t_Loss_alpha_0319_30_batchsize_1000__noterme.mat','r_t','v_t','Loss_Rec','alpha');)invalid file because of false save in the latter experiment.


Train:Yosemite, test Liberty. 5， 10 without mining 14.4%
Train:Yosemite, test Notredeme. 5， 10 without mining 8.4%

Train:Yosemite, test Liberty. 5， 10 without mining 13.9% (50 epoches)
Train:Yosemite, test Notredeme. 5， 10 without mining 7.8%(50 epoches)


Train:Notredeme, test Yosemite. 1， 10 with mining 11.73% 
Train:Notredeme, test Liberty. 1， 10 with mining 12.47%


2) Test the influence of coefficients before C_s and C_ds, namely the relative ratio of similar and dissimilar cost.

3)test tahn, with 3 and 10, it is not good behaved....

4) computation time: 11 hours in each training case

5) experiment again on the comparison of 4 different methods.
save('Loss_stdgradientdescent1.mat','Loss_Rec');
save('Loss_stdgradient1_mommentum.mat','Loss_Rec');
save('Loss_Rmsprop_nesterov_mommentum.mat','Loss_Rec');
save('Loss_Neterov_Gradient1.mat','Loss_Rec');